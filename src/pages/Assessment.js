import React, { useState, useEffect, useRef } from 'react';
import { useNavigate } from 'react-router-dom';
import './Assessment.css';
import * as faceapi from 'face-api.js';

const Assessment = () => {
  const navigate = useNavigate();
  const [answer, setAnswer] = useState('');
  const [isVideoEnded, setIsVideoEnded] = useState(false);
  const [faceDetected, setFaceDetected] = useState(false);
  const [cameraActive, setCameraActive] = useState(false);
  const [modelLoaded, setModelLoaded] = useState(false);
  const [emotion, setEmotion] = useState('');
  const [emotionStats, setEmotionStats] = useState(null);
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [analysisTime, setAnalysisTime] = useState(0);
  const [isAnalysisReady, setIsAnalysisReady] = useState(false);
  const [emotionHistory, setEmotionHistory] = useState([]);
  const [dominantEmotion, setDominantEmotion] = useState(null);
  
  const MIN_ANALYSIS_TIME = 5; // ìµœì†Œ 5ì´ˆ ë™ì•ˆ ë¶„ì„
  const analysisTimer = useRef(null);

  const videoRef = useRef(null);
  const cameraRef = useRef(null);
  const canvasRef = useRef(null);
  const streamRef = useRef(null);
  const speechRecognitionRef = useRef(null);

  // face-api.js ëª¨ë¸ ë¡œë“œ
  useEffect(() => {
    const loadModels = async () => {
      try {
        const MODEL_URL = process.env.PUBLIC_URL + '/weights';
        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
        setModelLoaded(true);
      } catch (error) {
        console.error('ëª¨ë¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ:', error);
      }
    };

    loadModels();
  }, []);

  // ì¹´ë©”ë¼ ì‹œì‘
  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: 640,
          height: 480,
          facingMode: 'user'
        }
      });
      
      if (cameraRef.current) {
        cameraRef.current.srcObject = stream;
        streamRef.current = stream;
        setCameraActive(true);
      }
    } catch (err) {
      console.error("ì¹´ë©”ë¼ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", err);
    }
  };

  // ì¹´ë©”ë¼ ì¤‘ì§€
  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      setCameraActive(false);
    }
  };

  // ì–¼êµ´ ê°ì§€
  const detectFace = async () => {
    if (!cameraRef.current || !canvasRef.current || !modelLoaded || !cameraActive) return;

    const video = cameraRef.current;
    const canvas = canvasRef.current;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) return;
    
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    try {
      const detections = await faceapi
        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceExpressions();

      const ctx = canvas.getContext('2d', { willReadFrequently: true });
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      if (detections && detections.length > 0) {
        setFaceDetected(true);
        const currentEmotions = detections[0].expressions;
        setEmotionStats(currentEmotions);
        
        // í˜„ì¬ ê°ì • ìƒíƒœì—ì„œ ê°€ì¥ ê°•í•œ ê°ì • ì°¾ê¸°
        const strongestEmotion = Object.entries(currentEmotions).reduce((prev, current) => 
          prev[1] > current[1] ? prev : current
        )[0];
        
        setEmotion(strongestEmotion);
        
        // ìŒì„± ì¸ì‹ ì¤‘ì¼ ë•Œë§Œ ê°ì • ê¸°ë¡
        if (isListening) {
          setEmotionHistory(prev => [...prev, currentEmotions]);
        }
        
        // ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        detections.forEach(detection => {
          if (detection.box) {
            const { box } = detection;
            ctx.strokeStyle = '#00ff88';
            ctx.lineWidth = 2;
            ctx.strokeRect(box.x, box.y, box.width, box.height);
          }
        });
      } else {
        setFaceDetected(false);
        setEmotion('');
        setEmotionStats(null);
      }

      if (cameraActive) {
        requestAnimationFrame(detectFace);
      }
    } catch (error) {
      console.error('ì–¼êµ´ ê°ì§€ ì¤‘ ì—ëŸ¬ ë°œìƒ:', error);
      if (cameraActive) {
        requestAnimationFrame(detectFace);
      }
    }
  };

  // ìŒì„± ì¸ì‹ ì´ˆê¸°í™”
  useEffect(() => {
    if ('webkitSpeechRecognition' in window) {
      const recognition = new window.webkitSpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'ko-KR';

      recognition.onresult = (event) => {
        const transcript = Array.from(event.results)
          .map(result => result[0])
          .map(result => result.transcript)
          .join('');
        setTranscript(transcript);
        setAnswer(transcript); // ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ë‹µë³€ì— ë°˜ì˜
      };

      recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
      };

      speechRecognitionRef.current = recognition;
    }
  }, []);

  // ìŒì„± ì¸ì‹ ì‹œì‘/ì¤‘ì§€
  const toggleListening = () => {
    if (isListening) {
      speechRecognitionRef.current?.stop();
      setIsListening(false);
      clearInterval(analysisTimer.current);
      setIsAnalysisReady(true);
      
      // ê°ì • ë¶„ì„ ì™„ë£Œ ì‹œ ì§€ë°°ì ì¸ ê°ì • ê³„ì‚°
      if (emotionHistory.length > 0) {
        calculateDominantEmotion();
      } else if (emotionStats && emotion) {
        // í˜„ì¬ ê°ì • ìƒíƒœë¥¼ ì‚¬ìš©
        setDominantEmotion({
          emotion: emotion,
          score: emotionStats[emotion]
        });
      }
    } else {
      speechRecognitionRef.current?.start();
      setIsListening(true);
      setAnalysisTime(0);
      setIsAnalysisReady(false);
      setEmotionHistory([]);
      
      // ë¶„ì„ ì‹œê°„ íƒ€ì´ë¨¸ ì‹œì‘
      analysisTimer.current = setInterval(() => {
        setAnalysisTime(prev => {
          if (prev >= MIN_ANALYSIS_TIME) {
            setIsAnalysisReady(true);
            return prev;
          }
          return prev + 1;
        });
      }, 1000);
    }
  };

  // ì§€ë°°ì ì¸ ê°ì • ê³„ì‚°
  const calculateDominantEmotion = () => {
    if (emotionHistory.length === 0) return;

    // ëª¨ë“  ê°ì • ë°ì´í„° í•©ì‚°
    const totalEmotions = emotionHistory.reduce((acc, curr) => {
      Object.keys(curr).forEach(emotion => {
        acc[emotion] = (acc[emotion] || 0) + curr[emotion];
      });
      return acc;
    }, {});

    // í‰ê·  ê³„ì‚°
    Object.keys(totalEmotions).forEach(emotion => {
      totalEmotions[emotion] /= emotionHistory.length;
    });

    // ê°€ì¥ ê°•í•œ ê°ì • ì°¾ê¸°
    const dominant = Object.entries(totalEmotions).reduce((prev, current) => 
      prev[1] > current[1] ? prev : current
    );

    setDominantEmotion({
      emotion: dominant[0],
      score: dominant[1]
    });
  };

  // ê°ì • í•œê¸€ ë³€í™˜
  const translateEmotion = (emotion) => {
    const translations = {
      neutral: 'ë¬´í‘œì •',
      happy: 'í–‰ë³µ',
      sad: 'ìŠ¬í””',
      angry: 'í™”ë‚¨',
      fearful: 'ë‘ë ¤ì›€',
      disgusted: 'ì‹«ìŒ',
      surprised: 'ë†€ëŒ'
    };
    return translations[emotion] || emotion;
  };

  // ë¹„ë””ì˜¤ ë° ì¹´ë©”ë¼ ì´ˆê¸°í™”
  useEffect(() => {
    if (modelLoaded) {
      startCamera();
    }
    return () => stopCamera();
  }, [modelLoaded]);

  // ì¹´ë©”ë¼ í™œì„±í™” ì‹œ ì–¼êµ´ ê°ì§€ ì‹œì‘
  useEffect(() => {
    if (cameraRef.current && cameraActive && modelLoaded) {
      cameraRef.current.addEventListener('play', () => {
        detectFace();
      });
    }
  }, [cameraActive, modelLoaded]);

  // ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
  const handleSubmit = (e) => {
    if (e) e.preventDefault();

    if (!isAnalysisReady) {
      alert(`ê°ì • ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ ${MIN_ANALYSIS_TIME}ì´ˆ ì´ìƒ ë‹µë³€í•´ ì£¼ì„¸ìš”.`);
      return;
    }

    if (!faceDetected) {
      alert('ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.');
      return;
    }

    if (!emotion || !emotionStats) {
      alert('ê°ì • ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.');
      return;
    }

    navigate('/results', {
      state: {
        answer: transcript || '',
        dominantEmotion: {
          type: emotion,
          name: translateEmotion(emotion),
          score: Math.round(emotionStats[emotion] * 100)
        }
      }
    });
  };

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ íƒ€ì´ë¨¸ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (analysisTimer.current) {
        clearInterval(analysisTimer.current);
      }
    };
  }, []);

  return (
    <div className="assessment-container">
      <div className="video-section">
        <video
          ref={videoRef}
          controls
          onEnded={() => setIsVideoEnded(true)}
          className="question-video"
        >
          <source src="/video/daniquestionandanswer.mp4" type="video/mp4" />
          ë¸Œë¼ìš°ì €ê°€ ë¹„ë””ì˜¤ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        </video>
      </div>

      <div className="answer-section">
        <div className="voice-control">
          <button 
            className={`voice-control-btn ${isListening ? 'active' : ''}`}
            onClick={toggleListening}
          >
            {isListening ? (
              <>
                <span className="btn-icon">â¹</span>
                ìŒì„± ì¸ì‹ ì¤‘ì§€
                <span className="analysis-time">
                  {analysisTime < MIN_ANALYSIS_TIME 
                    ? `(${MIN_ANALYSIS_TIME - analysisTime}ì´ˆ ë‚¨ìŒ)` 
                    : '(ë¶„ì„ ì™„ë£Œ)'}
                </span>
              </>
            ) : (
              <>
                <span className="btn-icon">ğŸ¤</span>
                ìŒì„± ì¸ì‹ ì‹œì‘
              </>
            )}
          </button>
          
          {transcript && (
            <div className="transcript-display">
              <p className="transcript-text">{transcript}</p>
            </div>
          )}

          <button 
            className={`submit-button ${!isAnalysisReady ? 'disabled' : ''}`}
            onClick={handleSubmit}
            disabled={!isAnalysisReady}
          >
            ê²°ê³¼ ë³´ê¸°
          </button>
        </div>
      </div>

      <div className="analysis-section">
        <h3>ì‹¤ì‹œê°„ ë¶„ì„</h3>
        <div className="analysis-grid">
          <div className="analysis-card face-detection-card">
            <div className="card-header">
              <h4>ì–¼êµ´ ê°ì§€ & í‘œì • ë¶„ì„</h4>
              <div className="status-badge">
                {faceDetected ? (
                  <>
                    <span className="status-detected">ê°ì§€ë¨ âœ“</span>
                    <span className="status-emotion">
                      {emotion ? `ê°ì •: ${translateEmotion(emotion)}` : 'ê°ì • ë¶„ì„ ì¤‘...'}
                    </span>
                  </>
                ) : 'ëŒ€ê¸° ì¤‘...'}
              </div>
            </div>
            
            <div className="camera-container">
              <video
                ref={cameraRef}
                autoPlay
                playsInline
                muted
                className="camera-view"
              />
              <canvas
                ref={canvasRef}
                className="face-canvas"
              />
            </div>

            {emotionStats && (
              <div className="emotion-stats">
                <div className="stats-title">ê°ì • ìˆ˜ì¹˜:</div>
                {Object.entries(emotionStats).map(([emotion, value]) => (
                  <div key={emotion} className="stat-row">
                    <span>{translateEmotion(emotion)}:</span>
                    <span>{(value * 100).toFixed(2)}%</span>
                  </div>
                ))}
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};

export default Assessment; 